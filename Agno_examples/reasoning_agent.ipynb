{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Attempt <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> failed: The api_key client option must be set either by passing api_key to the client or by   \n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mWARNING \u001b[0m Attempt \u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m failed: The api_key client option must be set either by passing api_key to the client or by   \n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Failed after <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> attempts. Last error using <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OpenAIChat</span><span style=\"font-weight: bold\">(</span>gpt-4o<span style=\"font-weight: bold\">)</span>                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Failed after \u001b[1;36m1\u001b[0m attempts. Last error using \u001b[1;35mOpenAIChat\u001b[0m\u001b[1m(\u001b[0mgpt-4o\u001b[1m)\u001b[0m                                              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Reasoning error: The api_key client option must be set either by passing api_key to the client or by      \n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Reasoning error: The api_key client option must be set either by passing api_key to the client or by      \n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModelProviderError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/models/openai/chat.py:452\u001b[39m, in \u001b[36mOpenAIChat.invoke_stream\u001b[39m\u001b[34m(self, messages, response_format, tools, tool_choice)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.chat.completions.create(\n\u001b[32m    453\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    454\u001b[39m         messages=[\u001b[38;5;28mself\u001b[39m._format_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    455\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    456\u001b[39m         stream_options={\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m    457\u001b[39m         **\u001b[38;5;28mself\u001b[39m.get_request_params(response_format=response_format, tools=tools, tool_choice=tool_choice),\n\u001b[32m    458\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/models/openai/chat.py:126\u001b[39m, in \u001b[36mOpenAIChat.get_client\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m     client_params[\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.http_client\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAIClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/openai/_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelProviderError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magno\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIChat\n\u001b[32m      4\u001b[39m reasoning_agent = Agent(\n\u001b[32m      5\u001b[39m     model=OpenAIChat(\u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m     reasoning=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     markdown=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mreasoning_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSolve the trolley problem. Evaluate multiple ethical frameworks. \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInclude an ASCII diagram of your solution.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_full_reasoning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/agent/agent.py:6662\u001b[39m, in \u001b[36mAgent.print_response\u001b[39m\u001b[34m(self, message, session_id, user_id, messages, audio, images, videos, files, stream, stream_intermediate_steps, markdown, show_message, show_reasoning, show_full_reasoning, console, tags_to_include_in_markdown, knowledge_filters, **kwargs)\u001b[39m\n\u001b[32m   6659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[32m   6660\u001b[39m     live_log.update(Group(*panels))\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6665\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6666\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6667\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   6672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mknowledge_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknowledge_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6674\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6675\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   6676\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunResponseEvent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   6677\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_paused\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/agent/agent.py:821\u001b[39m, in \u001b[36mAgent._run_stream\u001b[39m\u001b[34m(self, run_response, run_messages, session_id, user_id, response_format, stream_intermediate_steps)\u001b[39m\n\u001b[32m    818\u001b[39m index_of_last_user_message = \u001b[38;5;28mlen\u001b[39m(run_messages.messages)\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# 2. Process model response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_model_response_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[38;5;66;03m# 3. Add the run to memory\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/agent/agent.py:2994\u001b[39m, in \u001b[36mAgent._handle_model_response_stream\u001b[39m\u001b[34m(self, run_response, run_messages, response_format, stream_intermediate_steps)\u001b[39m\n\u001b[32m   2991\u001b[39m     log_debug(\u001b[33m\"\u001b[39m\u001b[33mResponse model set, model response is not streamed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2992\u001b[39m     stream_model_response = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_event\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tools_for_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_functions_for_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtool_call_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_model_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_model_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_model_response_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreasoning_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreasoning_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3011\u001b[39m \u001b[38;5;66;03m# Determine reasoning completed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/models/base.py:762\u001b[39m, in \u001b[36mModel.response_stream\u001b[39m\u001b[34m(self, messages, response_format, tools, functions, tool_choice, tool_call_limit, stream_model_response)\u001b[39m\n\u001b[32m    759\u001b[39m stream_data = MessageData()\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_model_response:\n\u001b[32m    761\u001b[39m     \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response_stream(\n\u001b[32m    763\u001b[39m         messages=messages,\n\u001b[32m    764\u001b[39m         assistant_message=assistant_message,\n\u001b[32m    765\u001b[39m         stream_data=stream_data,\n\u001b[32m    766\u001b[39m         response_format=response_format,\n\u001b[32m    767\u001b[39m         tools=tools,\n\u001b[32m    768\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tool_choice,\n\u001b[32m    769\u001b[39m     )\n\u001b[32m    771\u001b[39m     \u001b[38;5;66;03m# Populate assistant message from stream data\u001b[39;00m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_data.response_content:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/models/base.py:724\u001b[39m, in \u001b[36mModel.process_response_stream\u001b[39m\u001b[34m(self, messages, assistant_message, stream_data, response_format, tools, tool_choice)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[33;03mProcess a streaming response from the model.\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    723\u001b[39m assistant_message.metrics.start_timer()\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_provider_response_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_populate_stream_data_and_assistant_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response_delta\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/agno/models/openai/chat.py:495\u001b[39m, in \u001b[36mOpenAIChat.invoke_stream\u001b[39m\u001b[34m(self, messages, response_format, tools, tool_choice)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    494\u001b[39m     log_error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError from OpenAI API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProviderError(message=\u001b[38;5;28mstr\u001b[39m(e), model_name=\u001b[38;5;28mself\u001b[39m.name, model_id=\u001b[38;5;28mself\u001b[39m.id) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mModelProviderError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "\n",
    "reasoning_agent = Agent(\n",
    "    model=OpenAIChat(id=\"gpt-4o\"),\n",
    "    reasoning=True,\n",
    "    markdown=True,\n",
    ")\n",
    "reasoning_agent.print_response(\n",
    "    \"Solve the trolley problem. Evaluate multiple ethical frameworks. \"\n",
    "    \"Include an ASCII diagram of your solution.\",\n",
    "    stream=True,\n",
    "    show_full_reasoning=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
